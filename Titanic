# -*- coding: utf-8 -*
"""
Created on Tue Jul 17 13:54:52 2018

@author: SinghK01
"""
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

#####Importing the dataset and takingelevant columns only
dataset_train = pd.read_csv('C:/Users/karandeep_singh01/Desktop/all/train.csv').iloc[:,:].values
X_train = np.column_stack((dataset_train[:,2], dataset_train[:,4:8], dataset_train[:,11]))
y_train = dataset_train[:,1]

dataset_test = pd.read_csv('C:/Users/karandeep_singh01/Desktop/all/test.csv').iloc[:,:].values
X_test_final = np.column_stack((dataset_test[:,1], dataset_test[:,3:7], dataset_test[:,10]))

#####Compensating for missing data
from sklearn.preprocessing import Imputer
"""******Accuracy pt.1 - Can change Strategy"""
#Replacing missing values for AGE with the mean age
imputer = Imputer(missing_values = "NaN", strategy = "mean", axis = 0)
imputer.fit(X_train[:,2:3])
X_train[:,2:3] = imputer.transform(X_train[:,2:3])
#Had to take [:,2:3] above because if I just took [:,2] it would throw a 
#DepricationWarning because it would be a 1-d array
imputer.fit(X_test_final[:,2:3])
X_test_final[:,2:3] = imputer.transform(X_test_final[:,2:3])
#Replacing values for EMBARKED with the most frequent(mode) Embarked value
X_train = pd.DataFrame(X_train).fillna(pd.DataFrame(X_train).mode().iloc[0])
X_test_final = pd.DataFrame(X_test_final).fillna(pd.DataFrame(X_test_final).mode().iloc[0])

#Verifying that the training dataset now has no NaNs
pd.DataFrame(X_train).isnull().sum()
pd.DataFrame(X_test_final).isnull().sum()

#####Encoding Categorical Data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
label_enc = LabelEncoder()
label_enc = label_enc.fit(X_train.iloc[:,1])
X_train.iloc[:,1] = label_enc.transform(X_train.iloc[:,1])
X_test_final.iloc[:,1] = label_enc.transform(X_test_final.iloc[:,1])

label_enc = label_enc.fit(X_train.iloc[:,5])
X_train.iloc[:,5] = label_enc.transform(X_train.iloc[:,5])
X_test_final.iloc[:,5] = label_enc.transform(X_test_final.iloc[:,5])

onehot_enc = OneHotEncoder(categorical_features = [5], sparse = False)
X_train = onehot_enc.fit_transform(X_train)
X_test_final = onehot_enc.fit_transform(X_test_final)

#####Applying Dummy variable trap avoidance
X_train = X_train[:,1:]
X_test_final = X_test_final[:,1:]

#####Feature scaling needs to be done as we are using Dimentionality reduction
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test_final = scaler.fit_transform(X_test_final)

######Applying PCA Dimensionality Reduction
#"""******Accuracy pt.2 - Can change Reduction Technique or can change the no. 
#of components"""
#from sklearn.decomposition import PCA
#pc = PCA(n_components = 3)
#X_train = pc.fit_transform(X_train)
#X_test_final = pc.transform(X_test_final)
#explained_variance = pc.explained_variance_ratio_

#####Splitting X_train to get some Test Data
from sklearn.cross_validation import train_test_split
X_train_initial, X_test_initial,y_train_initial,y_test_initial = train_test_split(X_train, y_train, test_size = 0.2,random_state = 0) 


#####$Model training Logistic Regssion
from sklearn.linear_model import LogisticRegression
classifier_LR = LogisticRegression(random_state = 0)

#####$Model training K-NN
from sklearn.neighbors import KNeighborsClassifier
classifier_KNN = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', 
                                      p = 2)
#####$Model training SVM
from sklearn.svm import SVC
classifier_SVM = SVC(C = 1.0, kernel = 'linear', random_state = 0)

#####$Model training Kernel SVM
from sklearn.svm import SVC
classifier_K_SVM = SVC(C = 1.0, kernel = 'rbf', gamma = 'auto', random_state = 0)

#####$Model training Naive Bayes
from sklearn.naive_bayes import GaussianNB
classifier_NB = GaussianNB()

#####$Model training Decision Tree
from sklearn.tree import DecisionTreeClassifier
classifier_DT = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)

#####$Model training Random Forest
from sklearn.ensemble import RandomForestClassifier
classifier_RF = RandomForestClassifier(n_estimators = 10, criterion='entropy',
                                       random_state = 0)


#####Importing certain libraries for future use
from sklearn.cross_validation import cross_val_score

#####Applying k-fold Cross Validation to find out best mean accuracies 
#    of various models
y_train_initial = np.asarray(y_train_initial, dtype = float )
y_test_initial = np.asarray(y_test_initial, dtype = float )
classifiers_list = [(classifier_LR,'classifier_LR'),(classifier_KNN,
                    'classifier_KNN'),(classifier_SVM,'classifier_SVM'),
                    (classifier_K_SVM,'classifier_K_SVM'),(classifier_NB, 
                    'classifier_NB'), (classifier_DT,'classifier_DT'), 
                    (classifier_RF,'classifier_RF')]
for clf in classifiers_list:
    #Fitting classifier to training data
    clf[0].fit(X_train_initial, y_train_initial)
    #Predicting output
    y_pred_initial = clf[0].predict(X_test_initial)
    #Calculating mean accuracy using k-fold
    accuracies = cross_val_score(estimator = clf[0], X = X_train_initial, 
                                 y = y_train_initial, cv = 10)
    mean_accuracy = accuracies.mean()
    print('Mean k-fold accuracy of '+ clf[1] + ' is ' + str(mean_accuracy))
